<!DOCTYPE html>
<html>
<head>
  <title>Classification et modèles de mélange</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <link rel="stylesheet" media="all" href="libs/ioslides-13.5.1/fonts/fonts.css">

  <link rel="stylesheet" media="all" href="libs/ioslides-13.5.1/theme/css/default.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="libs/ioslides-13.5.1/theme/css/phone.css">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Classification et modèles de mélange',
                        useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
              },

      // Author information
      presenters: [
            {
        name:  'FX Jollois' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

  </style>


  <style>
  slide img {
  	max-width: 100%;
  }
  </style>

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
            <p style="margin-top: 6px; margin-left: -2px;">CNAM - 16 mars 2016</p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>Plan</h2></hgroup><article  id="plan">

<ol>
<li>Problème de classification</li>
<li>Modèles de mélange</li>
<li>Nombre de classes</li>
<li>Applications</li>
</ol>

</article></slide><slide class=''><hgroup><h2>Classification</h2></hgroup><article  id="classification">

<ul>
<li>Réduction d&#39;un nuage de points d’un espace quelconque en un ensemble de représentants moins nombreux</li>
<li>Représentation simplifiée des données initiales : Méthode de réduction des données</li>
<li>Applications nombreuses</li>
<li>Deux grandes familles de classification :

<ul>
<li>par partitionnement</li>
<li>par hiérarchie</li>
</ul></li>
</ul>

<p><em>Notation</em> : Soit \(x\) une matrice de données \(n \times d\) définie par \(x = {x^j_i ; i \in I; j \in J}\), où \(I\) est un ensemble de \(n\) objets (lignes, observations, instances, individus) et \(J\) est un ensemble de \(d\) variables (colonnes, attributs).</p>

</article></slide><slide class=''><hgroup><h2>Partition</h2></hgroup><article  id="partition">

<p><strong>Définition</strong> : Une partition de \(I\) en \(s\) classes (\(s\) est supposé connu) est un ensemble de parties non vides \(z_1,\dots,z_s\) vérifiant :</p>

<ul>
<li>\(\forall k, k&#39; = 1,\ldots,s , k \neq k&#39;, z_k \cap z_{k&#39;} = \emptyset\),</li>
<li><p>\(\cup^s_{k = 1} z_k = I\)</p></li>
<li>Nombre de partitions possibles très important

<ul>
<li>1701 partitions possibles de 8 objets répartis en 4 classes</li>
</ul></li>
<li>Meilleure partition : problème très complexe</li>
<li><p>Partition optimale localement</p></li>
</ul>

<blockquote>
<p>On se place ici dans le cadre de partitions dites non-recouvrantes : un individu appartient à une et une seule classe</p>
</blockquote>

</article></slide><slide class=''><hgroup><h2>Partitionnement</h2></hgroup><article  id="partitionnement">

<ul>
<li>Classification directe</li>
<li>Algorithme

<ol>
<li>Initialisation : \(s\) points tirés au hasard pour les centres de gravité de chaque classe,</li>
<li>Affectation : On affecte les points à la classe la plus proche,</li>
<li>Représentation : On recalcule les nouveaux centres de gravité,</li>
<li>On répète les étapes d’affectation et de représentation jusqu’à la convergence de l’algorithme (i. e. plus de changement de le partition).</li>
</ol></li>
<li>Convergence assez rapide (moins de 20 itérations généralement)</li>
<li>Résultats dépendant de l&#39;initialisation</li>
<li>Nombre de classes devant être connu</li>
</ul>

</article></slide><slide class=''><hgroup><h2>\(k\)-means</h2></hgroup><article  id="k-means">

<ul>
<li>Critère à minimiser :\[
W(z,g) =  \sum_{k=1}^s \sum_{i \in z_k}  d^2(x_i,g_k)
\]</li>
<li>Somme des inerties intra-classes</li>
<li>Basé sur la distance euclidienne</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Inconvénients</h2></hgroup><article  id="inconvenients">

<ul>
<li>Données continues : \(k\)-means

<ul>
<li>Classes sphériques, et de même taille</li>
<li>Classes petites <em>vidées</em></li>
</ul></li>
<li>Données binaires :

<ul>
<li>Adaptation du critère de \(k\)-means</li>
<li>Contrainte sur les centres des classes (pas de moyenne, mais valeur \(0\) ou \(1\) la plus présente)</li>
</ul></li>
<li>Données catégorielles : \(k\)-modes

<ul>
<li>\(k\)-means avec la métrique du \(\chi^2\)</li>
<li>Problèmes similaires à \(k\)-means</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Fuzzy \(c\)-means</h2></hgroup><article  id="fuzzy-c-means">

<ul>
<li>Critère à minimiser :\[
J_m(\mu,g) =  \sum_{k=1}^s \sum_{i=1}^n (\mu_{ik})^m d^2(x_i,g_k)
\]</li>
<li>\(\mu = [\mu_{ik}]\) : degré d&#39;appartenance de \(i\) à la classe \(k\) (entre 0 et 1)</li>
<li>Un individu peut donc appartenir à plusieurs classes, avec un degré spécifique

<ul>
<li>\(\sum_k \mu_{ik} = 1\)</li>
</ul></li>
<li>Problèmes similaires à \(k\)-means pour les formes des classes et les proportions</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Hiérarchie</h2></hgroup><article  id="hierarchie">

<p>??</p>

</article></slide><slide class=''><hgroup><h2>Modèles de mélange</h2></hgroup><article  id="modeles-de-melange">

<ul>
<li>Distribution de probabilité : mélange de \(s\) distributions associées aux classes</li>
<li>Cas d’une variable continue, avec deux classes sont présentes</li>
</ul>

<p><img src="cnam-classif-modeles_files/figure-html/ex-modele-1.png" title="" alt="" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Densité de probabilité</h2></hgroup><article  id="densite-de-probabilite">

<ul>
<li>Tableau de données \(x\) considéré comme échantillon \((x_1, \ldots,x_n)\) i.i.d. de taille \(n\) d’une variable aléatoire avec la densité \(\varphi(x,\theta)\) définie par :\[
\varphi(x_i;\theta) = \sum_{k=1}^s p_k \varphi_k (x_i;\alpha_k)
\]</li>
<li>\(\varphi_k(x_i, \alpha_k)\) : densité de probabilité de la classe \(k\)</li>
<li>\(p_k\) : probabilité qu’un élément de l’échantillon suive la loi \(\varphi_k\) (proportions du mélange)

<ul>
<li>\(\forall k=1,\ldots,n, p_k \in ]0,1[\)</li>
<li>\(\sum_{k=1}^s p_k = 1\)</li>
</ul></li>
<li>\(\theta = (p_1, \ldots ,p_s; \alpha_1, \ldots ,\alpha_s)\) : paramètre du modèle de mélange</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Vraissemblance</h2></hgroup><article  id="vraissemblance">

<ul>
<li>Problème statistique : estimer les proportions des composants ( les \((p_k)\)) et les paramètres (les \((\alpha_k)\))</li>
<li>Utilisation de la log-vraisemblance :\[
L(x_1,\ldots,x_n;\theta) = \sum_{i=1}^n \log \left( \sum_{k=1}^s p_k \varphi_k (x_i;\alpha_k) \right)
\]</li>
<li>Pour la classification, chaque \(x_i\) appartiendra à une classe \(k\), tel que \(z_{ik} = 1\) (et 0 sinon)</li>
<li>Log-vraissemblance complétée :\[
L_c(x_1,\ldots,x_n;\theta) = \sum_{i=1}^n \sum_{k=1}^s z_{ik} \log\left( p_k \varphi_k (x_i;\alpha_k) \right)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Deux approches</h2></hgroup><article  id="deux-approches">

<ul>
<li>Approche <strong>Estimation</strong>

<ul>
<li>Estimation des paramètres du mélange</li>
<li>Déduction de la partition, avec la méthode du maximum a posteriori <em>MAP</em></li>
<li>Maximisation de la log-vraisemblance \(L(x,\theta)\)</li>
<li>Utilisation de l&#39;lagorithme <strong>EM</strong></li>
</ul></li>
<li>Approche <strong>Classification</strong>

<ul>
<li>Estimation conjointe des paramètres et de la partition</li>
<li>Maximisation de la log-vraisemblance classifiante \(L_C(x,\theta)\)</li>
<li>Utilisation de l&#39;algorithme <strong>CEM</strong></li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Algorithme EM</h2></hgroup><article  id="algorithme-em">

<ul>
<li><strong>EM</strong> : <em>Estimation-Maximisation</em></li>
<li>Algorithme :

<ol>
<li>Déterminer une situation initiale</li>
<li><strong>Estimation</strong> des probabilités a posteriori\[
t_{ik} = \frac{p_k \varphi_k (x_i;\alpha_k)}{\sum_{\ell=1}^s p_\ell \varphi_\ell (x_i;\alpha_\ell)}
\]</li>
<li><strong>Maximisation</strong> : calcul des paramètres du mélange\[
\begin{aligned}
    p_k &amp;= \frac{\sum_{i=1}^n t_{ik}}{n} \\
    \alpha_k &amp;= \mbox{dépendant du modèle}
\end{aligned}
\]</li>
<li>Itérer les étapes 2 et 3, jusqu&#39;à la convergence (évolution très faible de \(L\))</li>
</ol></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Algorithme CEM</h2></hgroup><article  id="algorithme-cem">

<ul>
<li><strong>CEM</strong> : <em>Classification EM</em></li>
<li>Ajout d&#39;une étape de classification dans <strong>EM</strong>

<ol>
<li>Déterminer une situation initiale</li>
<li><strong>Estimation</strong> des probabilités a posteriori \(t_{ik}\) (identique)</li>
<li><strong>Classification</strong> des individus avec la méthode du <em>MAP</em>\[
z_k = \{ i | t_{ik} = max_{\ell=1,\ldots,s} t_{i\ell} \}
\]</li>
<li><strong>Maximisation</strong> : calcul des paramètres du mélange\[
\begin{aligned}
    p_k &amp;= \frac{Card(z_k)}{n} \\
    \alpha_k &amp;= \mbox{dépendant du modèle}
\end{aligned}
\]</li>
<li>Itérer les étapes 2 à 4, jusqu&#39;à la convergence (évolution très faible de \(L_c\))</li>
</ol></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Compléments sur EM et CEM</h2></hgroup><article  id="complements-sur-em-et-cem">

<ul>
<li>Résultats dépendant fortement de l&#39;initialisation

<ul>
<li>Lancement avec des initialisations différentes</li>
<li>Récupération de la meilleure solution, selon \(L\) (ou \(L_c\))</li>
<li>Initialisation de <strong>EM</strong> avec la meilleure solution de <strong>CEM</strong></li>
</ul></li>
<li>Cas gaussien

<ul>
<li>Fuzzy \(c\)-means : <strong>EM</strong> avc contraintes sur le modèle</li>
<li>\(k\)-means : <strong>CEM</strong> avec contraintes sur le modèle</li>
</ul></li>
<li>Cas qualitatif :

<ul>
<li>\(k\)-modes : <strong>CEM</strong> avec contraintes sur le modèle</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Hiérarchique</h2></hgroup><article  id="hierarchique">

<p>??</p>

</article></slide><slide class=''><hgroup><h2>Données quantitatives</h2></hgroup><article  id="donnees-quantitatives">

<ul>
<li>En présence de données continues, utilisation du modèle gaussien</li>
<li>Densité de probabilité de la classe \(k\) :\[
\varphi(x_i;g_k,\Sigma_k) = (2\pi)^{-d/2} |\Sigma_k|^{-1/2} \exp \left( -\frac{1}{2} (x_i - g_k)&#39; \Sigma_k^{-1} (x_i - g_k) \right)
\]</li>
<li>\(g_k\) : moyenne des variables pour la classe \(k\)</li>
<li>\(\Sigma_k\) : matrice de variance-covariance de la classe \(k\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Décomposition de la matrice de variance</h2></hgroup><article  id="decomposition-de-la-matrice-de-variance">

<p>Expression de la matrice de variance en fonction de sa décomposition en valeurs propres\[
    \Sigma_k = \lambda_k D_k A_k&#39; D_k&#39;
\]</p>

<ul>
<li>\(\lambda_k = |\Sigma_k|^{1/d}\), détermine le <strong>volume</strong> de la classe</li>
<li>\(D_k\) : matrice des vecteurs propres, détermine l&#39;<strong>orientation</strong> de la classe</li>
<li>\(A_k\) : matrice diagonale (tel \(|A_k|=1\)), avec les valeurs propres de \(\Sigma_k\), détermine la <strong>forme</strong> de la classe</li>
</ul>

<p>En imposant des contraintes sur ces valeurs, utilisation de modèles plus ou moins parcimonieux</p>

</article></slide><slide class=''><hgroup><h2>14 modèles gaussiens retreints</h2></hgroup><article  id="modeles-gaussiens-retreints" class="center smaller">

<div class="columns-2">
<ul>
<li><strong>Famille générale</strong>

<ul>
<li>Volumes différents (\(\lambda_k\)) ou identiques (\(\lambda\))</li>
<li>Formes différentes (\(A_k\)) ou identiques (\(A\))</li>
<li>Orientations différentes (\(D_k\)) ou identiques (\(D\))</li>
</ul></li>
<li><strong>Famille diagonale</strong>

<ul>
<li>Matrices \(\Sigma_k\) diagonales</li>
<li>\(D_k\) : matrices de permutation</li>
</ul></li>
<li><strong>Famille sphérique</strong>

<ul>
<li>\(A_k\) : matrice identité (\(I_k\))</li>
<li>Formes sphériques</li>
</ul></li>
</ul>

<p><img src="cnam-classif-modeles-14modeles.png" alt="14 modèles de décomposition" style="height: 10cm"></p></div>

<p>Proportions \(p_k\) pouvant être contraintes à être identiques (donc 28 modèles en tout)</p>

</article></slide><slide class=''><hgroup><h2>Données qualitatives</h2></hgroup><article  id="donnees-qualitatives">

<ul>
<li>Utilisation du modèle des classes latentes</li>
<li>Densité de probabilité de la classe \(k\)\[
\varphi(x_i;\alpha_k) = \prod_{j=1}^d \prod_{e=1}{c_j} \left( a_k^{je} \right)^{x_i^{je}}
\]</li>
<li>Variable \(j\) à \(c_j\) modalités</li>
<li>\(x_i^{je} = 1\) si l&#39;individu \(i\) a la modalité \(e\) pour la variable \(j\)</li>
<li>\(a_k^{je}\) : probabilité de la modalité \(e\) de la variable \(j\) pour la classe \(z_k\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Restriction sur les paramètres</h2></hgroup><article  id="restriction-sur-les-parametres">

<ul>
<li>De manière identique au cas gaussien, possibilité de créer des modèles restreints</li>
<li>Modèle générale : une probabilité pour chaque modalité de chaque variable dans chaque classe</li>
<li>Restriction première :

<ul>
<li>\(m_k^j\) : modalité majoritaire pour la variable \(j\) dans la classe \(k\)</li>
<li>\(\varepsilon_k^j\) : probabilité d&#39;erreur</li>
<li>\(\delta(x,y) = 0\) si \(x=y\), et \(1\) sinon\[
\varphi(x_i;\alpha_k) = \prod_{j=1}^d (1 - \varepsilon_k^j)^{1 - \delta(x_i^j,m_k^j)} \left( \frac{\varepsilon_k^j}{c_j - 1} \right)^{\delta(x_i^j,m_k^j)}
\]</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>10 modèles des classes latentes restreints</h2></hgroup><article  id="modeles-des-classes-latentes-restreints" class="center smaller">

<div class="columns-2">
<ul>
<li>\(\alpha_k^{je}\) : une probabilité pour chaque modalité</li>
<li>\(\varepsilon_k^j\) : erreur spécifique à une classe et une variable</li>
<li>\(\varepsilon_k\) : erreur spécifique à une classe (et identique pour toutes les variables)</li>
<li>\(\varepsilon^j\) : erreur spécifique à une variable (et identique pour toutes les classes)</li>
<li>\(\varepsilon\) : erreur identique pour toutes les variables dans toutes les classes</li>
<li>\(p_k\) : proportions des classes différentes</li>
<li>\(p\) : proportions des classes identiques</li>
</ul>

<p><img src="cnam-classif-modeles-Quantimodeles.png" alt="Modèles restreints" style="height: 10cm"></p></div>

</article></slide><slide class=''><hgroup><h2>Nombre de classes</h2></hgroup><article  id="nombre-de-classes">

<p>Dans toutes ces méthodes, le nombre de classes doit être <strong>connu</strong>.</p>

<ul>
<li>Choix fait par un expert métier</li>
<li>Méthodes de recherche du nombre de classes

<ul>
<li>Utilisation de la vraisemblance seule impossible (conduit à une classe par objet)</li>
<li>Classification hiérarchique (inutilisable si trop d&#39;objets)</li>
<li>Critères de choix de modèles

<ul>
<li>(ici) basé sur une pénalisation de la vraisemblance\[
C(s) = -2 \left( L_{max}(s) + \gamma_C \times \nu(s) \right)
\]</li>
<li>\(L_{max}(s)\) : vraisemblance maximum pour \(s\) classes</li>
<li>\(\gamma_C\) : coefficient de pénalisation, spécifique à chaque critère \(C\)</li>
<li>\(\nu(s)\) : nombre de paramètres libres du modèle considéré</li>
</ul></li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>\(AIC\) et dérivés</h2></hgroup><article  id="aic-et-derives">

<ul>
<li>\(AIC\) : critère d&#39;information d&#39;Akaike (\(\gamma_{AIC} = 1\))\[
AIC(s) = -2L(s) + 2 \nu(s)
\]</li>
<li>\(AIC3\) : version modifiée (\(\gamma_{AIC3} = \frac{3}{2}\))\[
AIC3(s) = -2L(s) + 3 \nu(s)
\]</li>
<li>\(AWE\) : approximation de la solution exacte (\(\gamma_{AWE} = \frac{1}{2}\left( \frac{3}{2} + \log n\right)\))\[
AWE(s) = -2L(s) + \nu(s) \left( \frac{3}{2} + \log n\right)
\]</li>
<li>Critères finalement peu utilisés, mais pouvant donner de bons résultats (notemment \(AIC3\))</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Critères bayésiens</h2></hgroup><article  id="criteres-bayesiens">

<ul>
<li>\(BIC\) : estimation de la vraisemblance intégrée (et similaire aux précédents avec \(\gamma_{BIC}=\frac{\log n}{2}\))\[
BIC(s) = L(s) - \frac{\nu(s)}{2} \log n
\]</li>
<li>\(ICL\) : basée sur la vraissemblance intégrée complétée\[
ICL(s) = L_C(s) - n\sum_{k=1}^s p_k \log p_k - \frac{\nu(s)}{2} \log n + S(np_1, \ldots, np_s)
\]</li>
<li>\(ICL\) et principalement \(BIC\) sont les plus utilisés (comparaison de ces deux critères dans la suite)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Quelques liens d&#39;applications</h2></hgroup><article  id="quelques-liens-dapplications">

<ul>
<li>Packages <strong>R</strong>

<ul>
<li><a href='http://www.mixmod.org/' title=''>Rmixmod</a></li>
<li><a href='http://www.stat.washington.edu/mclust/' title=''>mclust</a></li>
</ul></li>
<li>Packages <strong>Python</strong>

<ul>
<li><a href='http://scikit-learn.org/stable/index.html' title=''>Scikit-learn</a></li>
<li><a href='http://www.pymix.org/pymix/' title=''>Pymix</a></li>
</ul></li>
<li>Procédure <a href='https://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_fmm_toc.htm' title=''><code>FMM</code></a> dans <strong>SAS</strong></li>
<li>Logiciel <a href='http://www.mixmod.org/' title=''><strong>mixmod</strong></a></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Exemple d&#39;applications</h2></hgroup><article  id="exemple-dapplications"></article></slide>


  <slide class="backdrop"></slide>

</slides>

<script src="libs/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
<script src="libs/ioslides-13.5.1/js/prettify/prettify.js"></script>
<script src="libs/ioslides-13.5.1/js/prettify/lang-r.js"></script>
<script src="libs/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
<script src="libs/ioslides-13.5.1/js/hammer.js"></script>
<script src="libs/ioslides-13.5.1/js/slide-controller.js"></script>
<script src="libs/ioslides-13.5.1/js/slide-deck.js"></script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
